---
markmap:
  initialExpandLevel: 2
---

# Phase 1.2: Architecture Overview

**Learning Objective:** Understand how Tunix components fit together and interact to enable LLM post-training.

---

## 1. High-Level System Architecture

### The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Tunix Library                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SFT    â”‚  â”‚    RL    â”‚  â”‚ Distill  â”‚  â”‚ Generate â”‚  â”‚
â”‚  â”‚  Module  â”‚  â”‚  Module  â”‚  â”‚  Module  â”‚  â”‚  Module  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚             â”‚              â”‚             â”‚         â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                       â”‚                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚  Core Trainer   â”‚                           â”‚
â”‚              â”‚   (PeftTrainer) â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                       â”‚                                     â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚       â”‚               â”‚               â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Models  â”‚    â”‚  Utils  â”‚    â”‚  Perf   â”‚              â”‚
â”‚  â”‚ Module  â”‚    â”‚ Module  â”‚    â”‚ Module  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚  JAX/Flax NNX  â”‚  â”‚ Pathways  â”‚
        â”‚   (Compute)    â”‚  â”‚(Distributed)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚    TPU    â”‚
                    â”‚  Hardware â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Philosophy

**Layered Architecture:**
1. **Application Layer:** High-level training paradigms (SFT, RL, Distillation)
2. **Core Training Layer:** Shared training infrastructure (PeftTrainer)
3. **Model Layer:** Model implementations and loading
4. **Framework Layer:** JAX, Flax NNX, Pathways
5. **Hardware Layer:** TPU execution

**Key Principles:**
- **Modularity:** Each component is self-contained and composable
- **Extensibility:** Easy to add new algorithms, models, strategies
- **Performance:** Optimized for TPU execution
- **Flexibility:** Support multiple training paradigms

---

## 2. Core Components Breakdown

### 2.1 PeftTrainer (Foundation)

**Location:** `tunix/sft/peft_trainer.py`

**Role:** The foundational trainer that ALL other trainers build upon

**Key Responsibilities:**
1. **Training Loop Management**
   - Execute train steps
   - Manage iteration and global steps
   - Handle gradient accumulation

2. **Model Management**
   - Parameter initialization
   - State management (params, optimizer state)
   - Model sharding across devices

3. **Checkpointing**
   - Save/restore model checkpoints
   - Save/restore optimizer state
   - Save/restore training state

4. **Metrics & Logging**
   - Log training metrics
   - Track training progress
   - Integration with logging backends

5. **Performance Optimization**
   - JIT compilation
   - Profiling integration
   - Memory optimization

**Core Components:**

```python
class PeftTrainer:
    model: nnx.Module           # The model being trained
    optimizer: nnx.Optimizer    # The optimizer (wraps optax)
    training_config: TrainingConfig
    
    # Key methods:
    def train(data_iterator) -> None
    def train_step(batch) -> loss
    def eval_step(batch) -> metrics
    def save_checkpoint() -> None
    def restore_checkpoint() -> None
```

**Training Flow:**

```
1. Initialize trainer with model + optimizer
   â†“
2. Load checkpoint (if resuming)
   â†“
3. For each training step:
   â”œâ”€ Get batch from iterator
   â”œâ”€ Prepare inputs (shard, tokenize)
   â”œâ”€ Run train_step (forward + backward)
   â”œâ”€ Accumulate gradients
   â”œâ”€ Update weights
   â”œâ”€ Log metrics
   â””â”€ Save checkpoint (if needed)
   â†“
4. Training complete
```

### 2.2 SFT Module

**Location:** `tunix/sft/`

**Components:**

1. **PeftTrainer** (`peft_trainer.py`)
   - Base trainer implementation
   - Supports full fine-tuning and LoRA/QLoRA

2. **DPOTrainer** (`dpo/dpo_trainer.py`)
   - Extends PeftTrainer
   - Implements Direct Preference Optimization
   - Uses preference pairs for training

3. **ORPOTrainer** (`dpo/dpo_trainer.py`)
   - Variant of DPO
   - Odds Ratio Preference Optimization

4. **Support Components:**
   - `checkpoint_manager.py` - Checkpoint management
   - `metrics_logger.py` - Logging infrastructure
   - `sharding_utils.py` - Model/data sharding
   - `progress_bar.py` - Training progress display
   - `profiler.py` - Performance profiling
   - `utils.py` - Common utilities

**Data Flow:**

```
Dataset â†’ DataLoader â†’ Batch
              â†“
        Tokenization
              â†“
         Sharding (across devices)
              â†“
     PeftTrainer.train_step()
              â†“
    Forward Pass â†’ Loss â†’ Gradients â†’ Update
              â†“
      Metrics Logging
```

### 2.3 RL Module

**Location:** `tunix/rl/`

**Architecture:** More complex than SFT due to RL requirements

**Key Components:**

1. **RLCluster** (`rl_cluster.py`)
   - **Central orchestrator** for RL training
   - Manages multiple models (actor, critic, reference, reward)
   - Handles different device meshes
   - Coordinates training and rollout

2. **RLLearner** (`rl_learner.py`)
   - Abstract base for RL algorithms
   - Manages rollout â†’ training cycle
   - Handles advantage computation
   - Coordinates data flow

3. **Algorithm Implementations:**
   - `ppo/ppo_learner.py` - PPO algorithm
   - `grpo/grpo_learner.py` - GRPO algorithm
   - `grpo/drgrpo_learner.py` - Divergence Regularized GRPO
   - `grpo/dapo_learner.py` - Direct Advantage Policy Optimization

4. **Rollout System** (`rollout/`)
   - `base_rollout.py` - Abstract rollout interface
   - `vanilla_rollout.py` - Standard JAX rollout
   - vLLM integration - Fast inference engine
   - SGLang integration - Structured generation

5. **Support Components:**
   - `trainer.py` - RL-specific trainer (extends PeftTrainer)
   - `common.py` - Shared RL data structures
   - `algorithm_config.py` - Algorithm configuration
   - `utils.py` - RL utilities

**RLCluster Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RLCluster                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Actor   â”‚  â”‚ Critic   â”‚  â”‚Reference â”‚        â”‚
â”‚  â”‚ Trainer  â”‚  â”‚ Trainer  â”‚  â”‚  Model   â”‚        â”‚
â”‚  â”‚  (Policy)â”‚  â”‚ (Value)  â”‚  â”‚  (Fixed) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚             â”‚              â”‚               â”‚
â”‚  [Mesh: Actor]  [Mesh: Actor]  [Mesh: Ref]       â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Reward  â”‚  â”‚     Rollout Engine       â”‚      â”‚
â”‚  â”‚  Model   â”‚  â”‚ (vLLM/SGLang/Vanilla)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â”‚             â”‚                              â”‚
â”‚  [Mesh: Reward] [Mesh: Rollout]                  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RL Training Flow:**

```
1. RLCluster initialization
   â”œâ”€ Load actor model (policy to train)
   â”œâ”€ Load critic model (value estimator, for PPO)
   â”œâ”€ Load reference model (original policy, frozen)
   â”œâ”€ Load reward model (optional)
   â””â”€ Setup rollout engine
   â†“
2. For each training iteration:
   â”‚
   â”œâ”€ ROLLOUT PHASE (inference):
   â”‚  â”œâ”€ Get prompts from dataset
   â”‚  â”œâ”€ Generate completions with current policy
   â”‚  â”œâ”€ Compute rewards for completions
   â”‚  â””â”€ Compute advantages (how good each completion was)
   â”‚
   â”œâ”€ TRAINING PHASE:
   â”‚  â”œâ”€ Shuffle rollout data
   â”‚  â”œâ”€ Split into mini-batches
   â”‚  â”œâ”€ For each mini-batch:
   â”‚  â”‚  â”œâ”€ Compute policy loss
   â”‚  â”‚  â”œâ”€ Compute value loss (PPO only)
   â”‚  â”‚  â”œâ”€ Update actor (and critic)
   â”‚  â”‚  â””â”€ Clip updates to prevent large changes
   â”‚  â””â”€ Log metrics
   â”‚
   â””â”€ Checkpoint models
   â†“
3. Training complete
```

**Colocated vs Disaggregated Setup:**

**Colocated:** All components on same mesh
```
Actor, Critic, Reference, Rollout â†’ Same TPU Pod
```
- Simpler setup
- Good for smaller models
- Less communication overhead

**Disaggregated:** Different meshes for different roles
```
Actor + Critic â†’ Training Mesh (e.g., v4-128)
Rollout + Reference â†’ Inference Mesh (e.g., v4-64)
```
- Better resource utilization
- Async rollout possible
- Scales better for large models

### 2.4 Distillation Module

**Location:** `tunix/distillation/`

**Components:**

1. **DistillationTrainer** (`distillation_trainer.py`)
   - Extends PeftTrainer
   - Manages teacher and student models
   - Applies distillation strategies

2. **Strategies** (`strategies/`)
   - Different distillation approaches
   - Logit matching, attention transfer, etc.
   - Feature extraction and projection

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DistillationTrainer               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Teacher  â”‚      â”‚ Student  â”‚      â”‚
â”‚  â”‚  Model   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Model   â”‚      â”‚
â”‚  â”‚ (Frozen) â”‚      â”‚(Training)â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â”‚                 â”‚              â”‚
â”‚       â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚   â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  Strategy    â”‚                     â”‚
â”‚  â”‚  (Logit/     â”‚                     â”‚
â”‚  â”‚  Attention/  â”‚                     â”‚
â”‚  â”‚  Feature)    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Loss Compute â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training Flow:**

```
1. Initialize teacher (frozen) and student (trainable)
   â†“
2. For each batch:
   â”œâ”€ Forward pass through teacher
   â”œâ”€ Extract teacher outputs/features
   â”œâ”€ Forward pass through student
   â”œâ”€ Extract student outputs/features
   â”œâ”€ Compute distillation loss via strategy
   â”œâ”€ Backprop through student only
   â””â”€ Update student parameters
   â†“
3. Student learns to mimic teacher
```

### 2.5 Generation Module

**Location:** `tunix/generate/`

**Purpose:** Text generation and sampling during inference/rollout

**Components:**

1. **Samplers:**
   - `sampler.py` - Standard JAX sampler
   - `vllm_sampler.py` - vLLM integration
   - `sglang_jax_sampler.py` - SGLang integration
   - `base_sampler.py` - Abstract interface

2. **Support:**
   - `tokenizer_adapter.py` - Unified tokenizer interface
   - `beam_search.py` - Beam search implementation
   - `mappings.py` - Token mapping utilities
   - `vllm_async_driver.py` - Async vLLM driver

**Why Multiple Samplers?**

| Sampler | Use Case | Speed | Features |
|---------|----------|-------|----------|
| **Vanilla (JAX)** | Simple use, full control | Medium | Full customization |
| **vLLM** | High-throughput inference | Fast | PagedAttention, batching |
| **SGLang** | Structured generation | Fast | Constrained decoding, DSL |

**Generation Pipeline:**

```
Input Prompt
     â†“
Tokenization
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sampler Engine â”‚
â”‚  (JAX/vLLM/    â”‚
â”‚   SGLang)      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Autoregressive Generation:
  â”œâ”€ Forward pass
  â”œâ”€ Sample next token
  â”œâ”€ Append to sequence
  â””â”€ Repeat until EOS
     â†“
Detokenization
     â†“
Output Text
```

### 2.6 Models Module

**Location:** `tunix/models/`

**Purpose:** Model definitions and loading utilities

**Components:**

1. **Model Families:**
   - `gemma/` - Gemma model implementation
   - `gemma3/` - Gemma 3 implementation
   - `llama3/` - Llama 3 implementation
   - `qwen2/` - Qwen 2 implementation
   - `qwen3/` - Qwen 3 implementation

2. **Utilities:**
   - `automodel.py` - Automatic model loading
   - `safetensors_loader.py` - Load from SafeTensors
   - `safetensors_saver.py` - Save to SafeTensors
   - `naming.py` - Parameter naming conventions
   - `dummy_model_creator.py` - Testing utilities

**Model Loading Flow:**

```
Model Path/ID
     â†“
AutoModel.from_pretrained()
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Detect model type   â”‚
â”‚ (Gemma/Llama/Qwen)  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load architecture   â”‚
â”‚ (config.json)       â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load weights        â”‚
â”‚ (safetensors/       â”‚
â”‚  pytorch_model.bin) â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard across devicesâ”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
Ready Model (nnx.Module)
```

### 2.7 CLI Module

**Location:** `tunix/cli/`

**Purpose:** Command-line interface for training

**Components:**
- `config.py` - Configuration parsing
- `base_config.yaml` - Default configurations
- `grpo_main.py` - GRPO training entry point

**Configuration System:**

```
YAML Config Files
     â†“
OmegaConf Parser
     â†“
Validated Config Objects
     â†“
Trainer Initialization
```

**Config Composition:**

```yaml
# base_config.yaml (defaults)
model:
  name: "google/gemma-2b"
  
training:
  max_steps: 1000
  learning_rate: 1e-4

# user_config.yaml (overrides)
training:
  max_steps: 5000
  
# Command line (final overrides)
$ tunix train --config base_config.yaml,user_config.yaml \
    --training.learning_rate=5e-5
```

---

## 3. Data Flow Through System

### 3.1 SFT Data Flow

```
Raw Dataset
     â†“
Dataset Loading (TF Datasets/Grain)
     â†“
Tokenization
     â†“
Batching
     â†“
Sharding across devices
     â†“
PeftTrainer.train_step()
     â”‚
     â”œâ”€ Forward: model(input_tokens)
     â”œâ”€ Loss: cross_entropy(logits, targets)
     â”œâ”€ Backward: grad(loss)
     â””â”€ Update: optimizer.update(grads)
     â†“
Metrics Logging
     â†“
Checkpoint Saving
```

### 3.2 RL Data Flow

```
Prompt Dataset
     â†“
RLCluster.rollout()
     â”‚
     â”œâ”€ Load prompts
     â”œâ”€ Generate completions (inference)
     â”œâ”€ Compute rewards
     â””â”€ Compute advantages
     â†“
TrainExample batch
     â†“
RLLearner.train()
     â”‚
     â”œâ”€ Shuffle data
     â”œâ”€ Split into mini-batches
     â””â”€ For each mini-batch:
         â”‚
         â”œâ”€ Compute policy loss
         â”œâ”€ Compute value loss (PPO)
         â”œâ”€ Update actor/critic
         â””â”€ KL regularization
     â†“
Metrics Logging
     â†“
Checkpoint Saving
```

### 3.3 Distillation Data Flow

```
Training Dataset
     â†“
Tokenization & Batching
     â†“
DistillationTrainer.train_step()
     â”‚
     â”œâ”€ Teacher forward (inference)
     â”‚   â””â”€ Extract features/logits
     â”‚
     â”œâ”€ Student forward (training)
     â”‚   â””â”€ Extract features/logits
     â”‚
     â”œâ”€ Strategy.compute_loss()
     â”‚   â””â”€ Compare teacher vs student
     â”‚
     â”œâ”€ Backward through student
     â””â”€ Update student parameters
     â†“
Metrics Logging
     â†“
Checkpoint Saving
```

---

## 4. Component Interaction Patterns

### 4.1 Trainer Hierarchy

```
           PeftTrainer
           (Base class)
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
DPOTrainer  DistillationTrainer  RL Trainer
    â”‚                             â”‚
ORPOTrainer                      â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚               â”‚
                    PPOLearner      GRPOLearner
                                        â”‚
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚               â”‚
                          DrGRPOLearner    DAPOLearner
```

**Inheritance Benefits:**
- Code reuse (checkpointing, metrics, etc.)
- Consistent API across training paradigms
- Easy to extend with new algorithms

### 4.2 Model Management Pattern

```
User Code
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Model Specification   â”‚
â”‚ (path or config)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AutoModel.load()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚  Sharding â”‚
    â”‚  Strategy â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Distributed Model      â”‚
â”‚  (across devices)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
     Trainer Usage
```

### 4.3 Logging Pattern

```
Training Event
     â†“
MetricsLogger.log()
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Logger broadcasts   â”‚
â”‚  to all backends     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        â”‚        â”‚        â”‚
   â–¼        â–¼        â–¼        â–¼
Console  TensorBoard WandB  Custom
Backend  Backend     Backend Backend
```

**Protocol-based design allows pluggable backends**

### 4.4 Checkpoint Pattern

```
Trainer State:
â”œâ”€ Model parameters
â”œâ”€ Optimizer state
â”œâ”€ Training step counter
â”œâ”€ Data iterator state
â””â”€ Random seeds

     â†“
CheckpointManager.save()
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orbax Checkpoint    â”‚
â”‚  (efficient format)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   Saved to disk
       â”‚
CheckpointManager.restore()
       â†“
Trainer resumes seamlessly
```

---

## 5. Multi-Host Distributed Training Architecture

### 5.1 Device Mesh Concept

**Logical view:**
```
Mesh shape: (2, 4)  # 2 FSDP replicas, 4 tensor parallel
Device IDs:
[0, 1, 2, 3]
[4, 5, 6, 7]
```

**Physical mapping:**
```
TPU Pod: 2 hosts Ã— 4 chips each = 8 total chips

Host 0: Chips [0, 1, 2, 3]
Host 1: Chips [4, 5, 6, 7]

Mesh coordinates data/model sharding
```

### 5.2 Sharding Strategies

**FSDP (Fully Sharded Data Parallel):**
```
Model split across devices:

Device 0: Layers 0-3
Device 1: Layers 4-7
Device 2: Layers 8-11
Device 3: Layers 12-15

Each device:
- Holds full optimizer state for its layers
- Processes different data batch
- All-gathers parameters when needed
```

**Tensor Parallelism:**
```
Single layer split across devices:

Linear layer: [4096, 4096]
Device 0: [4096, 1024]
Device 1: [4096, 1024]
Device 2: [4096, 1024]
Device 3: [4096, 1024]
```

**Combined (FSDP + TP):**
```
Mesh: (fsdp=2, tensor=4)

Layer 0-7:  FSDP replica 0, split across TP 0-3
Layer 8-15: FSDP replica 1, split across TP 0-3
```

### 5.3 Communication Patterns

**Collective Operations:**

1. **All-Reduce:** Gradients aggregation
```
Each device: local gradients
     â†“
All-Reduce sum
     â†“
Each device: averaged gradients
```

2. **All-Gather:** Parameter reconstruction
```
Device 0: Params 0-3
Device 1: Params 4-7
     â†“
All-Gather
     â†“
All devices: Full params 0-7
```

3. **Reduce-Scatter:** Optimizer state distribution
```
All devices: Full gradients
     â†“
Reduce-Scatter
     â†“
Each device: Subset of reduced gradients
```

---

## 6. Memory Management

### 6.1 Memory Components

**During Training:**
```
Total Memory = Model Params + Optimizer State + Gradients + Activations

Example (7B model, bf16, Adam):
- Parameters: 14 GB
- Optimizer (Adam): 28 GB (2x params)
- Gradients: 14 GB
- Activations: varies by batch size
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~56 GB + activations
```

**FSDP Reduction:**
```
With 8-way FSDP:
Per-device memory: ~7 GB + activations/8

Enables training larger models!
```

### 6.2 QLoRA Memory Savings

```
Full Fine-tuning:
â”œâ”€ All parameters trainable: 100%
â”œâ”€ Full optimizer state: 100%
â””â”€ Memory: VERY HIGH

LoRA:
â”œâ”€ Base frozen: ~95% params
â”œâ”€ LoRA trainable: ~5% params
â”œâ”€ Optimizer state: 5% only
â””â”€ Memory: ~25% of full

QLoRA:
â”œâ”€ Base quantized (4-bit): ~25% memory
â”œâ”€ LoRA trainable (16-bit): ~5% params
â”œâ”€ Optimizer state: 5% only
â””â”€ Memory: ~15% of full
```

---

## 7. Performance Optimization Layers

### 7.1 Compilation (XLA/JIT)

```
Python Code
     â†“
JAX traces function
     â†“
XLA Compiler
     â†“
Optimized TPU kernels
     â†“
Cached for reuse
```

**What JIT optimizes:**
- Operator fusion
- Memory layout optimization
- Constant folding
- Dead code elimination

### 7.2 Micro-batching

```
Global batch: 128 samples
Micro-batch size: 32

Process in 4 micro-batches:
â”œâ”€ Micro-batch 1: Forward + Backward
â”œâ”€ Micro-batch 2: Forward + Backward
â”œâ”€ Micro-batch 3: Forward + Backward
â”œâ”€ Micro-batch 4: Forward + Backward
â””â”€ Aggregate gradients â†’ Single update

Reduces peak memory!
```

### 7.3 Profiling Integration

```
Training Loop
     â†“
Profiler captures:
â”œâ”€ Computation time per step
â”œâ”€ Memory usage
â”œâ”€ Communication time
â”œâ”€ Compilation time
â””â”€ Device utilization
     â†“
Export traces
     â†“
Analyze bottlenecks
```

---

## 8. Extension Points

### 8.1 Adding New Algorithms

```python
# Extend RLLearner for new RL algorithm
class MyCustomRLLearner(RLLearner):
    def _generate_and_compute_advantage(self, ...):
        # Custom advantage computation
        pass
    
    def _compute_policy_loss(self, ...):
        # Custom loss function
        pass
```

### 8.2 Adding New Models

```python
# Implement model architecture
class MyModel(nnx.Module):
    def __init__(self, config):
        # Define layers
        pass
    
    def __call__(self, inputs):
        # Forward pass
        pass

# Register with AutoModel
# Add to tunix/models/
```

### 8.3 Custom Distillation Strategies

```python
class MyStrategy(BaseStrategy):
    def compute_loss(self, teacher_out, student_out):
        # Custom distillation loss
        pass
```

### 8.4 Custom Logging Backends

```python
class MyBackend:
    def log_scalar(self, event, value, **kwargs):
        # Custom logging logic
        pass
    
    def close(self):
        pass

# Use with MetricsLoggerOptions
```

---

## 9. Architecture Diagrams

### 9.1 Complete RL Training Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Application                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RLLearner                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Training Coordination Loop                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚            â”‚                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚         â”‚  Rollout Phase  â”‚   â”‚ Training Phase  â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    RLCluster     â”‚   â”‚   Actor Trainer    â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚                    â”‚
        â”‚  â”‚  Rollout   â”‚  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  Engine    â”‚  â”‚   â”‚  â”‚ PeftTrainer  â”‚ â”‚
        â”‚  â”‚(vLLM/JAX)  â”‚  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ Reference  â”‚  â”‚
        â”‚  â”‚   Model    â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  Reward    â”‚  â”‚
        â”‚  â”‚   Model    â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Request Flow Diagram

```
User Request: "Fine-tune Gemma on my data"
         â†“
    PeftTrainer(model, optimizer, config)
         â†“
    trainer.train(data_iterator)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Training Loop Start   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Get next batch        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Shard across devices  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Forward pass (JIT)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Compute loss          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Backward pass (grad)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  All-reduce gradients  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Optimizer update      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Log metrics           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Save checkpoint?      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Check if done         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
         Repeat or End
```

---

## ğŸ¯ Phase 1.2 Checklist

- [ ] Understand overall system architecture
- [ ] Know role of PeftTrainer as foundation
- [ ] Grasp how SFT module works
- [ ] Understand RL module complexity and RLCluster
- [ ] Know data flow through each training paradigm
- [ ] Understand multi-host distributed training
- [ ] Familiar with memory management strategies
- [ ] Ready for deep dive into JAX/TPU technologies (Phase 1.3)

---

**Previous:** [Phase 1.1 - Core Concepts](Phase_1_1_Core_Concepts.md)  
**Next:** [Phase 1.3 - Key Technologies Deep Dive](Phase_1_3_Key_Technologies.md)
